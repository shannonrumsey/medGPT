# medGPT

The architecture of this model [1,2] is similar to that defined in the *Attention Is All You Need* paper [3]. There is no cross-attention, and the normalization layers occur before the attention mechanism instead of after. Additionally, an extra normalization layer is added right before the classifier. Many of the parameters align with those in the original paper; however, the learning rate for each parameter group utilizes a cosine annealing schedule. 

The model is trained using the MedQuAD dataset [4, 5]. The MedQuAD dataset consists of medical questions and answers. It has a total of 16,413 rows, many of which have multiple answers for the same question. After manual inspection, it was decided to keep only the first answer correpsonding to the question, effectively decreasing the dataset size by roughly half. After combining the questions and answers into sequences, many had a length of 500 words, though some were as long as 3,500 words. Keeping CPU capabilities in mind, only sequences with 500 words in length or shorter were considered for training.

![Distrbution of answer lengths.](https://github.com/shannonrumsey/medGPT/Figure_1.png)

When applicable, the model utilizes Distributed Data Parallel to speed up processing [6]. During training, the model generates hypotheses for the context "What is (are) Glaucoma?" and produces four answer candidates. These generated answers are compared to the actual answers from the MedQuad dataset using the SacreBLEU metric [7]. SacreBLEU is a score that reflects how similar the generated output is to the expected output. 

### Example output generated by the model:

Question: *What are the symptoms of Viral Hepatitis: A through E and Beyond?*

Expected Output: * Symptoms include, jaundice, which causes a yellowingof the skin and eyes, fatigue, abdominal pain, loss of appetite, nausea, vomiting, diarrhea, low grade fever, headache However, some people do not have symptoms.*

Snippet of Model Response: * country a and can patients have, taking system system Some Some (. cancer treatment Cancer to of type being'sI new to out are include the. out being being being section cancer from is These Cancer-*


### Perfomance 

Performance when training on ~14,000 lines of data (due to CPU processing) and 5 warmup steps + 20 steps:

| Step | Step 5   | Step 19 (Last Step) |
| :---:   | :---: | :---: |
| Val Loss | 10.6247  | 10.3385 |
| Loss | 10.5684   | 10.2138  |
| BLEU | 0.0000 | 0.0000 |

### Next Steps

Potential next steps would be to train on more data, train on more steps, and incorporate more layers, attention heads, and embedding dimensions in the model. On previous training runs, the BLEU score increased to about ~0.012; hopefully with these modifications, the BLEU score will increase again.

References:

[1] Karpathy, A. (2024). NanoGPT Training Script. [GitHub repository](https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py).

[2] Smith, R. (2023). Medical NLP Overview. [NCBI article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10654385/).

[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., Polosukhin, I. (2017). Attention Is All You Need. [arXiv preprint arXiv:1706.03762](https://arxiv.org/pdf/1706.03762).

[4] Ben Abacha, A., & Demner-Fushman, D. (2019). A Question-Entailment Approach to Question Answering. [BMC Bioinformatics, 20(1), 511:1–511:23].(https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3119-4).

[5] Miller, J. P. (2023). LayoutLM Dataset. [Kaggle dataset](https://www.kaggle.com/datasets/jpmiller/layoutlm/data).

[6] Bhansali, N. (2023). Data Parallelism with PyTorch on CPUs. [Medium article](https://medium.com/@nishantbhansali80/data-parallel-with-pytorch-on-cpus-3e89312db6c0).

[7] Nlplanet. (2023). Understanding BLEU Metric by Examples. [Medium article](https://medium.com/nlplanet/two-minutes-nlp-learn-the-bleu-metric-by-examples-df015ca73a86).







